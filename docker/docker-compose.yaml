version: '2.3'

services:
  triton-server-base:
    build:
      context: ../
      dockerfile: ./docker/Dockerfile
    shm_size: '10gb'
    runtime: nvidia
    volumes:
      - ../models:/models
    ports:
        - 8000:8000
        - 8001:8001
        - 8002:8002

  triton-server-down:
    extends:
      service: triton-server-base
    command: tail -f /dev/null

  triton-server-pyannote:
    extends:
      service: triton-server-base
    command: >
      tritonserver
      --log-verbose 1
      --model-control-mode=explicit
      --model-repository /models
      --model-load-thread-count=1
      --load-model=pyannote_diarization

  triton-server-dummy:
    extends:
      service: triton-server-base
    command: >
      tritonserver
      --log-verbose 1
      --model-control-mode=explicit
      --model-repository /models
      --load-model=dummy

  triton-server-mel:
    extends:
      service: triton-server-base
    command: >
      tritonserver
      --log-verbose 1
      --model-control-mode=explicit
      --model-repository /models
      --load-model=whisper_mel_worker
      --load-model=large_v2_whisper_encoder
      --load-model=mel_ensemble